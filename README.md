# Ocena rzetelnoÅ›ci artykuÅ‚Ã³w (Fake News) â€“ regresja i interpretacja

Projekt wykorzystujÄ…cy dataset FakeHealth do przewidywania rzetelnoÅ›ci artykuÅ‚Ã³w zdrowotnych za pomocÄ… regresji i interpretacji modeli ML.

## Dataset

**FakeHealth** to zbiÃ³r danych zawierajÄ…cy:
- **HealthStory**: ~1638 artykuÅ‚Ã³w z rÃ³Å¼nych ÅºrÃ³deÅ‚ medialnych
- **HealthRelease**: ~599 komunikatÃ³w prasowych uniwersytetÃ³w

KaÅ¼dy artykuÅ‚ zawiera:
- PeÅ‚ny tekst artykuÅ‚u
- ProfesjonalnÄ… recenzjÄ™ z ocenami rzetelnoÅ›ci (rating 1-5)
- 10 kryteriÃ³w oceny jakoÅ›ci dziennikarstwa medycznego
- SzczegÃ³Å‚owe wyjaÅ›nienia ocen

**Å¹rÃ³dÅ‚o**: [FakeHealth Repository](https://github.com/EnyanDai/FakeHealth)  
**Paper**: [Ginger Cannot Cure Cancer: Battling Fake Health News](https://arxiv.org/abs/2002.00837)

---

## ğŸš€ Szybki start

### Krok 1: Instalacja zaleÅ¼noÅ›ci

```bash
pip install -r requirements.txt
```

Minimalne wymagania:
```bash
pip install pandas numpy scikit-learn matplotlib seaborn
```

### Krok 2: Przygotowanie danych

```bash
python prepare_data.py
```

Skrypt automatycznie:
- Wczyta wszystkie artykuÅ‚y z folderÃ³w `dataset/content/HealthStory` i `dataset/content/HealthRelease`
- PoÅ‚Ä…czy je z recenzjami z `dataset/reviews/`
- Utworzy 4 pliki wynikowe:
  - `HealthStory_combined.json` - peÅ‚ne dane HealthStory (z kryteriami)
  - `HealthStory_combined.csv` - uproszczona wersja CSV
  - `HealthRelease_combined.json` - peÅ‚ne dane HealthRelease
  - `HealthRelease_combined.csv` - uproszczona wersja CSV

### Krok 3: Uruchomienie analizy

#### Opcja A: Jupyter Notebook (Zalecane)
OtwÃ³rz i uruchom `RandomForest_TF-IDF_Linguistic.ipynb` w Jupyter / VS Code / Google Colab:
- Zawiera peÅ‚nÄ… analizÄ™ z wizualizacjami
- 3 modele: Regresja, Binary Classification, Multi-Task
- Feature engineering: TF-IDF + 25 cech linguistic
- InterpretacjÄ™ wynikÃ³w i wyjaÅ›nialnoÅ›Ä‡

#### Opcja B: Skrypt Python
```bash
python test_analysis.py
```

---

## ğŸ“ Struktura projektu

```
.
â”œâ”€â”€ dataset/                                      # Oryginalne dane FakeHealth
â”‚   â”œâ”€â”€ content/
â”‚   â”‚   â”œâ”€â”€ HealthStory/                         # ~1638 artykuÅ‚Ã³w (pliki JSON)
â”‚   â”‚   â””â”€â”€ HealthRelease/                       # ~599 artykuÅ‚Ã³w (pliki JSON)
â”‚   â””â”€â”€ reviews/
â”‚       â”œâ”€â”€ HealthStory.json                     # Recenzje dla HealthStory
â”‚       â””â”€â”€ HealthRelease.json                   # Recenzje dla HealthRelease
â”‚
â”œâ”€â”€ RandomForest_TF-IDF_Linguistic.ipynb         # â­ GÅ‚Ã³wny notebook z analizÄ…
â”œâ”€â”€ results_RandomForest_TF-IDF_Linguistic.json  # Wyniki modeli
â”œâ”€â”€ selected_linguistic_features.txt             # Lista 25 cech linguistic
â”‚
â”œâ”€â”€ prepare_data.py                              # Skrypt Å‚Ä…czÄ…cy content + reviews
â”œâ”€â”€ test_analysis.py                             # PrzykÅ‚adowa analiza i modele ML
â”œâ”€â”€ requirements.txt                             # Wymagane biblioteki
â””â”€â”€ README.md                                    # Ten plik
```

---

## Struktura danych wynikowych

Po uruchomieniu `prepare_data.py` kaÅ¼dy rekord w plikach `*_combined.json` zawiera:

### Dane z artykuÅ‚u:
- `news_id` - unikalny identyfikator
- `url` - adres URL artykuÅ‚u
- `title` - tytuÅ‚ artykuÅ‚u
- `text` - **peÅ‚ny tekst artykuÅ‚u** (do analizy NLP)
- `authors` - lista autorÃ³w
- `publish_date` - data publikacji (timestamp)
- `keywords` - sÅ‚owa kluczowe
- `source` - ÅºrÃ³dÅ‚o artykuÅ‚u

### Dane z recenzji:
- `rating` - **ocena rzetelnoÅ›ci (1-5)** - zmienna celu do regresji! â­
  - 1 = najgorszy (fake news)
  - 5 = najlepszy (rzetelny)
- `review_title` - tytuÅ‚ recenzji
- `description` - krÃ³tki opis problemu z artykuÅ‚em
- `reviewers` - lista recenzentÃ³w
- `category` - kategoria ÅºrÃ³dÅ‚a (np. "The Guardian", "University news release")
- `tags` - tagi tematyczne
- `review_summary` - podsumowanie recenzji
- `why_this_matters` - dlaczego to ma znaczenie

### Kryteria oceny (do interpretacji modelu):
- `criteria` - lista 10 kryteriÃ³w, kaÅ¼de zawiera:
  - `question` - pytanie (np. "Czy artykuÅ‚ omawia koszty?")
  - `answer` - odpowiedÅº: "Satisfactory" / "Not Satisfactory" / "Not Applicable"
  - `explanation` - szczegÃ³Å‚owe wyjaÅ›nienie oceny
  
- `num_satisfactory` - liczba speÅ‚nionych kryteriÃ³w
- `num_not_satisfactory` - liczba niespeÅ‚nionych kryteriÃ³w
- `num_not_applicable` - liczba kryteriÃ³w nieaplikowalnych

### 10 kryteriÃ³w oceny jakoÅ›ci dziennikarstwa medycznego:
1. Czy artykuÅ‚ omawia **koszty** interwencji?
2. Czy **kwantyfikuje korzyÅ›ci**?
3. Czy omawia **zagroÅ¼enia/skutki uboczne**?
4. Czy ocenia **jakoÅ›Ä‡ dowodÃ³w naukowych**?
5. Czy nie przesadza z chorobÄ… (**disease-mongering**)?
6. Czy uÅ¼ywa **niezaleÅ¼nych ÅºrÃ³deÅ‚**?
7. Czy **porÃ³wnuje z alternatywami**?
8. Czy ustala **dostÄ™pnoÅ›Ä‡** interwencji?
9. Czy wspomina o **nowoÅ›ci vs. faktycznej innowacji**?
10. Czy identyfikuje **konflikty interesÃ³w**?

---

## Cel projektu

### Problem
Przewidywanie rzetelnoÅ›ci artykuÅ‚Ã³w zdrowotnych na podstawie tekstu. Projekt wykorzystuje trzy podejÅ›cia:

### PodejÅ›cie 1: Single-Task (Regresja)
- Przewidywanie ciÄ…gÅ‚ego **ratingu rzetelnoÅ›ci** (0-5)
- Proste, ale niska wyjaÅ›nialnoÅ›Ä‡

### PodejÅ›cie 2: Single-Task (Klasyfikacja Binarna)
- Klasyfikacja: **Fake News** (rating < 3) vs **Reliable** (rating â‰¥ 3)
- Lepsza accuracy, ale wciÄ…Å¼ brak szczegÃ³Å‚Ã³w

### PodejÅ›cie 3: Multi-Task (Przewidywanie 10 KryteriÃ³w) **GÅÃ“WNE**
- Przewidywanie, ktÃ³re z **10 kryteriÃ³w jakoÅ›ci dziennikarstwa** zostaÅ‚y zÅ‚amane
- **Naturalna wyjaÅ›nialnoÅ›Ä‡**: "ArtykuÅ‚ jest fake, bo zÅ‚amaÅ‚ kryteria: C1, C3, C6"
- ZgodnoÅ›Ä‡ z intencjÄ… autorÃ³w FakeHealth dataset
- KaÅ¼de kryterium = osobna klasyfikacja binarna (0=Not Satisfactory, 1=Satisfactory)

---

## ğŸ”¬ Zaimplementowane modele

### Features (1025 cech):
- **TF-IDF** (1000 features): Bag-of-Words z title + text
  - max_features=1000
  - ngram_range=(1, 2) - unigramy i bigramy
  - stop_words='english'
- **25 cech linguistic** z `selected_linguistic_features.txt`:
  - Cechy stylistyczno-jÄ™zykowe (20): Adjective, Adverb, Modal, Negation, Conditional, itp.
  - Cechy medyczno-jÄ™zykowe (5): biomedical_terms, commercial_terms, url_count

### Model 1: Random Forest Regressor (Single-Task)
- **Zadanie**: Przewidywanie ratingu (0-5)
- **Model**: Random Forest Regressor (100 drzew, max_depth=20)
- **Interpretacja**: Feature importance pokazuje najwaÅ¼niejsze sÅ‚owa/cechy
- **Wyniki**: RÂ²=0.23, RMSE=1.04, MAE=0.83

### Model 2: Random Forest Classifier (Single-Task)
- **Zadanie**: Klasyfikacja binarna fake/reliable
- **Model**: Random Forest Classifier (100 drzew, class_weight='balanced')
- **Interpretacja**: Feature importance
- **Wyniki**: Accuracy=0.74, F1=0.84, Precision=0.76, Recall=0.95
- âš ï¸ **Problem**: Model sÅ‚abo identyfikuje fake news (recall=21% dla fake), za to bardzo dobrze znajduje reliable (recall=95%). Klasyfikuje wiÄ™kszoÅ›Ä‡ fake newsÃ³w (79%) jako reliable - typowy problem niezbalansowanego datasetu

### Model 3: Multi-Output Random Forest â­ **GÅÃ“WNY MODEL**
- **Zadanie**: Przewidywanie 10 kryteriÃ³w jakoÅ›ci (kaÅ¼de: 0=Not Satisfactory, 1=Satisfactory)
- **Model**: MultiOutputClassifier z Random Forest (class_weight='balanced')
- **Interpretacja**: 
  - **Naturalna wyjaÅ›nialnoÅ›Ä‡** - dokÅ‚adnie wiemy, ktÃ³re kryteria zostaÅ‚y zÅ‚amane
  - Feature importance dla kaÅ¼dego kryterium osobno
- **Wyniki**: Åšrednia Accuracy=0.73, Åšrednia F1=0.70, Åšredni Recall=0.70
  - Najlepsze kryterium: C10 (Konflikty interesÃ³w) - F1=0.96
  - Najtrudniejsze: C2 (Kwantyfikacja korzyÅ›ci) - F1=0.40

---

## ğŸ“ˆ Metryki oceny

### Dla Regresji (Model 1):
- **RÂ² Score**: JakoÅ›Ä‡ dopasowania modelu (0-1, wyÅ¼szy = lepszy)
  - Training: 0.81, Test: **0.23** (moÅ¼liwy overfitting)
- **RMSE**: Root Mean Squared Error (niÅ¼szy = lepszy)
  - Test: **1.04** punktu ratingu
- **MAE**: Mean Absolute Error - Å›redni bÅ‚Ä…d w punktach ratingu
  - Test: **0.83** punktu ratingu

### Dla Klasyfikacji Binarnej (Model 2):
- **Accuracy**: Odsetek poprawnych klasyfikacji
  - Training: 0.98, Test: **0.74**
- **Precision**: Jaki % przewidzianych "reliable" jest rzeczywiÅ›cie reliable
  - Test: **0.76** (dla reliable), **0.63** (dla fake)
- **Recall**: Jaki % rzeczywistych przypadkÃ³w model znalazÅ‚
  - Test: **0.95** dla reliable (âœ… bardzo dobry - model znajduje 95% reliable)
  - Test: **0.21** dla fake (âš ï¸ sÅ‚aby - model znajduje tylko 21% fake newsÃ³w)
- **F1-Score**: Åšrednia harmoniczna precision i recall
  - Test: **0.84** (dla reliable), **0.31** (dla fake)
- **ROC-AUC**: ZdolnoÅ›Ä‡ modelu do rozrÃ³Å¼niania klas
  - Test: **0.72**
- âš ï¸ **Confusion Matrix**: Z 92 fake newsÃ³w tylko 19 sklasyfikowano poprawnie, a 73 bÅ‚Ä™dnie jako reliable (79% bÅ‚Ä…d!)

### Dla Multi-Task (Model 3) â­:
- **Accuracy per kryterium**: Odsetek poprawnych predykcji dla danego kryterium
  - Åšrednia: **0.73** (od 0.58 dla C7 do 0.92 dla C10)
- **F1-Score per kryterium**: Åšrednia harmoniczna precision i recall
  - Åšrednia: **0.70** (od 0.40 dla C2 do 0.96 dla C10)
- **Recall per kryterium**: Jaki % rzeczywistych "Satisfactory" model znalazÅ‚
  - Åšrednia: **0.70**

---

## âš ï¸ Uwagi i ograniczenia

### Ograniczenia datasetu:
- Rating jest **subiektywny** (ocena ekspertÃ³w), ale konsystentny
- Dataset jest **niezbalansowany** - wiÄ™cej artykuÅ‚Ã³w z niskim ratingiem
- **JÄ™zyk angielski** - modele mogÄ… nie dziaÅ‚aÄ‡ dla innych jÄ™zykÃ³w
- **Rozmiar** - ~2200 artykuÅ‚Ã³w (nie jest bardzo duÅ¼y dla deep learning)

### Statystyki datasetu:

**HealthStory** (~1638 artykuÅ‚Ã³w):
- ArtykuÅ‚y z rÃ³Å¼nych ÅºrÃ³deÅ‚ medialnych (gazety, portale)
- Recenzje profesjonalne z HealthNewsReview.org
- WiÄ™ksza rÃ³Å¼norodnoÅ›Ä‡ ÅºrÃ³deÅ‚ i stylÃ³w

**HealthRelease** (~599 artykuÅ‚Ã³w):
- GÅ‚Ã³wnie komunikaty prasowe uniwersytetÃ³w
- Bardziej homogenna grupa
- CzÄ™sto bardziej "marketingowe" podejÅ›cie

---

## ğŸ“š Literatura i ÅºrÃ³dÅ‚a

- [FakeHealth Paper (arXiv)](https://arxiv.org/abs/2002.00837) - Opis datasetu i metodologii
- [HealthNewsReview.org](https://www.healthnewsreview.org/) - Å¹rÃ³dÅ‚o recenzji i kryteriÃ³w
- [Scikit-learn Documentation](https://scikit-learn.org/) - Machine learning w Python
- [SHAP Documentation](https://shap.readthedocs.io/) - Interpretacja modeli
- [LIME Documentation](https://lime-ml.readthedocs.io/) - Lokalne wyjaÅ›nienia
