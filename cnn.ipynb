{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "997c9b0a62d1223f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T23:00:55.679842Z",
     "start_time": "2025-11-10T23:00:47.179705Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "import wordcloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e9f5e15a32e3072",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:09:53.994787Z",
     "start_time": "2025-11-10T01:09:43.654954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Wczytano dane z JSON\n",
      "Liczba rekordÃ³w: 1638\n",
      "Kolumny: ['news_id', 'title', 'text', 'rating', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10']\n",
      "================================================================================\n",
      "PODSUMOWANIE LABELI\n",
      "RATING: (1638,)  (min=0, max=5)\n",
      "BINARY: (1638,)  â†’ 1178 reliable, 460 fake\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\NQW483\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czyszczenie tekstÃ³w (to moÅ¼e potrwaÄ‡ kilka minut)...\n",
      "================================================================================\n",
      "Zapisano wszystko do jednego pliku: HealthStory_cleaned.csv\n",
      " Kolumny: ['news_id', 'title', 'text', 'rating', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'binary_label']\n",
      " Wiersze: 1638\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Wczytanie JSON i ekstrakcja kryteriÃ³w\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def extract_from_healthnews_json(json_path):\n",
    "    \"\"\"\n",
    "    Wczytuje dane z pliku JSON (lista rekordÃ³w lub pojedynczy rekord)\n",
    "    i zwraca DataFrame z tekstem, ratingiem i 10 kryteriami (Satisfactory = 1, Not = 0).\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    \n",
    "    records = []\n",
    "    for article in data:\n",
    "        record = {\n",
    "            'news_id': article.get('news_id'),\n",
    "            'title': article.get('title'),\n",
    "            'text': article.get('text'),\n",
    "            'rating': article.get('rating'),\n",
    "        }\n",
    "\n",
    "        # Ekstrakcja 10 kryteriÃ³w\n",
    "        criteria = article.get('criteria', [])\n",
    "        for i, crit in enumerate(criteria, start=1):\n",
    "            answer = crit.get('answer', '').strip()\n",
    "            if answer.lower().startswith('satisfactory'):\n",
    "                record[f'C{i}'] = 1\n",
    "            elif answer.lower().startswith('not'):\n",
    "                record[f'C{i}'] = 0\n",
    "            else:\n",
    "                record[f'C{i}'] = 1  # \"Not Applicable\" â†’ 1\n",
    "        \n",
    "        # BrakujÄ…ce kryteria uzupeÅ‚niamy jako Satisfactory\n",
    "        for j in range(len(criteria) + 1, 11):\n",
    "            record[f'C{j}'] = 1\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Wczytanie danych\n",
    "# --------------------------------------------------------------------------\n",
    "df = extract_from_healthnews_json(\"HealthStory_combined.json\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Wczytano dane z JSON\")\n",
    "print(f\"Liczba rekordÃ³w: {len(df)}\")\n",
    "print(f\"Kolumny: {list(df.columns)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Przygotowanie labels\n",
    "# --------------------------------------------------------------------------\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "df = df.dropna(subset=['text', 'rating']).reset_index(drop=True)\n",
    "\n",
    "# Binary label\n",
    "df['binary_label'] = (df['rating'] >= 3).astype(int)\n",
    "\n",
    "print(\"PODSUMOWANIE LABELI\")\n",
    "print(f\"RATING: {df['rating'].shape}  (min={df['rating'].min()}, max={df['rating'].max()})\")\n",
    "print(f\"BINARY: {df['binary_label'].shape}  â†’ {df['binary_label'].sum()} reliable, {(df['binary_label']==0).sum()} fake\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Czyszczenie tekstÃ³w\n",
    "# --------------------------------------------------------------------------\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in text.split() if w not in english_stopwords])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "print(\"Czyszczenie tekstÃ³w (to moÅ¼e potrwaÄ‡ kilka minut)...\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Zapis do pliku\n",
    "# --------------------------------------------------------------------------\n",
    "cleaned_path = \"HealthStory_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Zapisano wszystko do jednego pliku: {cleaned_path}\")\n",
    "print(f\" Kolumny: {list(df.columns)}\")\n",
    "print(f\" Wiersze: {len(df)}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18ee20ea3a6aef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T23:01:10.623918Z",
     "start_time": "2025-11-10T23:01:00.223003Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- 1a. WordCloud dla wizualizacji najczÄ™stszych sÅ‚Ã³w ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwordcloud\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      5\u001b[39m df_clean = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mHealthStory_cleaned.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# --- 1a. WordCloud dla wizualizacji najczÄ™stszych sÅ‚Ã³w ---\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "# Pozytywne recenzje\n",
    "positive_text = \" \".join(df_clean[df_clean['rating'] >= 3]['text'].astype(str))\n",
    "# Negatywne recenzje\n",
    "negative_text = \" \".join(df_clean[df_clean['rating'] < 3]['text'].astype(str))\n",
    "\n",
    "# Funkcja do generowania i wyÅ›wietlania wordclouda\n",
    "def plot_wordcloud(text, title):\n",
    "    wc = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis'\n",
    "    ).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "plot_wordcloud(positive_text, \"NajczÄ™stsze sÅ‚owa w recenzjach pozytywnych\")\n",
    "plot_wordcloud(negative_text, \"NajczÄ™stsze sÅ‚owa w recenzjach negatywnych\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9769d58f48cf9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T22:54:08.234490Z",
     "start_time": "2025-11-09T22:54:08.083999Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba 0 i 1: {0: 460, 1: 1178}\n"
     ]
    }
   ],
   "source": [
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "y = (df_clean['rating'] >= 3).astype(int).values\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "count_dict = dict(zip(unique, counts))\n",
    "print(\"Liczba 0 i 1:\", count_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:20:01.554982Z",
     "start_time": "2025-11-10T01:09:55.055048Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN/TEST SPLIT\n",
      "Train: 1291  Test: 323\n",
      "Train label counts: [366 925]\n",
      "Test  label counts: [ 91 232]\n",
      "============================================================\n",
      "ğŸ“ Maksymalna dÅ‚ugoÅ›Ä‡ sekwencji w danych: 2238\n",
      "ğŸ”¹ Wczytywanie FastText embeddings (to moÅ¼e chwilÄ™ potrwaÄ‡)...\n",
      "âœ… Znaleziono embeddingi dla 17,008 / 19,818 sÅ‚Ã³w.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NQW483\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚     \u001b[38;5;34m5,945,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d            â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> (22.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,945,400\u001b[0m (22.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> (22.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,945,400\u001b[0m (22.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 398ms/step - accuracy: 0.7049 - loss: 0.6101 - val_accuracy: 0.7183 - val_loss: 0.5849\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 405ms/step - accuracy: 0.7134 - loss: 0.5804 - val_accuracy: 0.7183 - val_loss: 0.5754\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 421ms/step - accuracy: 0.7196 - loss: 0.5526 - val_accuracy: 0.7183 - val_loss: 0.5734\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 422ms/step - accuracy: 0.7335 - loss: 0.5305 - val_accuracy: 0.7183 - val_loss: 0.5801\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 400ms/step - accuracy: 0.7653 - loss: 0.4986 - val_accuracy: 0.7245 - val_loss: 0.5652\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 392ms/step - accuracy: 0.7963 - loss: 0.4479 - val_accuracy: 0.7214 - val_loss: 0.5666\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 415ms/step - accuracy: 0.8443 - loss: 0.4036 - val_accuracy: 0.7183 - val_loss: 0.5786\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 422ms/step - accuracy: 0.8559 - loss: 0.3575 - val_accuracy: 0.7214 - val_loss: 0.5726\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 412ms/step - accuracy: 0.9055 - loss: 0.2985 - val_accuracy: 0.7245 - val_loss: 0.5889\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 414ms/step - accuracy: 0.9334 - loss: 0.2324 - val_accuracy: 0.7368 - val_loss: 0.5953\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 76ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š WYNIKI MODELU CNN\n",
      "Train Accuracy: 0.9853\n",
      "Test  Accuracy: 0.7368\n",
      "Precision: 0.7634\n",
      "Recall:    0.9181\n",
      "F1-Score:  0.8337\n",
      "ROC-AUC:   0.5964\n",
      "============================================================\n",
      "âœ… Model zapisany jako cnn_fasttext_full_unlimited.h5\n",
      "\n",
      "Wyniki zapisane do: results_CNN_Embedding.json\n",
      "Analiza zakoÅ„czona pomyÅ›lnie!\n"
     ]
    }
   ],
   "source": [
    "# --- PARAMETRY ---\n",
    "EMBEDDING_DIM = 300\n",
    "FASTTEXT_PATH = 'cc.en.300.vec'\n",
    "\n",
    "# --- 1. Wczytanie danych ---\n",
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "df_clean = df_clean.dropna(subset=['text'])\n",
    "df_clean = df_clean[df_clean['text'].str.strip() != \"\"]\n",
    "\n",
    "texts = df_clean['text'].astype(str).tolist()\n",
    "y = (df_clean['rating'] >= 3).astype(int).values\n",
    "\n",
    "# --- 2. PodziaÅ‚ train/test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(f\"Train: {len(X_train)}  Test: {len(X_test)}\")\n",
    "print(\"Train label counts:\", np.bincount(y_train))\n",
    "print(\"Test  label counts:\", np.bincount(y_test))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 3. Tokenizacja bez ograniczeÅ„ ---\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(len(seq) for seq in X_train_seq + X_test_seq)\n",
    "print(f\"ğŸ“ Maksymalna dÅ‚ugoÅ›Ä‡ sekwencji w danych: {max_length}\")\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "X_train_seq = np.array(X_train_seq, dtype=np.int32)\n",
    "X_test_seq = np.array(X_test_seq, dtype=np.int32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "# --- 4. Embeddingi FastText ---\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "if os.path.exists(FASTTEXT_PATH):\n",
    "    print(\"ğŸ”¹ Wczytywanie FastText embeddings (to moÅ¼e chwilÄ™ potrwaÄ‡)...\")\n",
    "    embedding_index = {}\n",
    "    with open(FASTTEXT_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = f.readline().strip().split()\n",
    "        if len(first_line) != 2 or not all(x.isdigit() for x in first_line):\n",
    "            f.seek(0)\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) < EMBEDDING_DIM + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:EMBEDDING_DIM + 1], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_index:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "            found += 1\n",
    "    print(f\"âœ… Znaleziono embeddingi dla {found:,} / {num_words:,} sÅ‚Ã³w.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Brak pliku FastText â€” losowa inicjalizacja embeddingÃ³w.\")\n",
    "    embedding_matrix = np.random.normal(size=(num_words, EMBEDDING_DIM)).astype('float32')\n",
    "\n",
    "# --- 5. Model CNN ---\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=False\n",
    "    ),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 6. Trening ---\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 7. Ewaluacja ---\n",
    "y_train_pred = (model.predict(X_train_seq) > 0.5).astype(int).ravel()\n",
    "y_test_pred = (model.predict(X_test_seq) > 0.5).astype(int).ravel()\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "rec_test = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "f1_test = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š WYNIKI MODELU CNN\")\n",
    "print(f\"Train Accuracy: {acc_train:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc_test:.4f}\")\n",
    "print(f\"Precision: {prec_test:.4f}\")\n",
    "print(f\"Recall:    {rec_test:.4f}\")\n",
    "print(f\"F1-Score:  {f1_test:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc_test:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 8. Zapis modelu ---\n",
    "model.save(\"cnn_fasttext_full_unlimited.h5\")\n",
    "print(\"âœ… Model zapisany jako cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "# --- 14. Zapisz wyniki do pliku JSON ---\n",
    "results_summary = {\n",
    "    'dataset': 'HealthStory',\n",
    "    'n_samples': len(df_clean),\n",
    "    'n_features': len(tokenizer.word_index),\n",
    "    'model': 'CNN + FastText embeddings',\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'accuracy': float(acc_train)\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': float(acc_test),\n",
    "            'precision': float(prec_test),\n",
    "            'recall': float(rec_test),\n",
    "            'f1': float(f1_test),\n",
    "            'roc_auc': float(auc_test)\n",
    "        },\n",
    "        'history': {\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'val_accuracy': history.history['val_accuracy']\n",
    "        }\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'trainable_embeddings': False\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = 'results_CNN_Embedding.json'\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "with open(\"results_CNN_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "print(f\"\\nWyniki zapisane do: {json_path}\")\n",
    "print(\"Analiza zakoÅ„czona pomyÅ›lnie!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f440580b75d8a3c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T22:59:03.176950Z",
     "start_time": "2025-11-10T22:59:03.055957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wczytano plik HealthStory_cleaned.csv\n",
      "Liczba rekordÃ³w: 1638\n",
      "Kolumny: ['news_id', 'title', 'text', 'rating', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'binary_label']\n",
      "Trening: 1310 prÃ³bek\n",
      "Test: 328 prÃ³bek\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NQW483\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d_1          â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_max_pooling1d_1          â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)            â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 568ms/step - accuracy: 0.1061 - loss: 0.6512 - val_accuracy: 0.0091 - val_loss: 0.6079\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 600ms/step - accuracy: 0.1053 - loss: 0.6159 - val_accuracy: 0.2043 - val_loss: 0.6063\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 592ms/step - accuracy: 0.1427 - loss: 0.6010 - val_accuracy: 0.1402 - val_loss: 0.6051\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 566ms/step - accuracy: 0.1328 - loss: 0.5825 - val_accuracy: 0.1311 - val_loss: 0.6039\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 597ms/step - accuracy: 0.1649 - loss: 0.5529 - val_accuracy: 0.1555 - val_loss: 0.5943\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 572ms/step - accuracy: 0.1672 - loss: 0.5014 - val_accuracy: 0.1585 - val_loss: 0.5828\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 582ms/step - accuracy: 0.1840 - loss: 0.4393 - val_accuracy: 0.1006 - val_loss: 0.5750\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 555ms/step - accuracy: 0.1511 - loss: 0.3812 - val_accuracy: 0.1189 - val_loss: 0.5725\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 545ms/step - accuracy: 0.1435 - loss: 0.3298 - val_accuracy: 0.1463 - val_loss: 0.5736\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 565ms/step - accuracy: 0.1473 - loss: 0.2870 - val_accuracy: 0.0640 - val_loss: 0.5806\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 73ms/step\n",
      "\n",
      " WYNIKI DLA KAÅ»DEGO KRYTERIUM:\n",
      "\n",
      "Kryterium                                       Accuracy         F1     Recall\n",
      "================================================================================\n",
      "C1: Koszty interwencji                            0.8171     0.5833     0.5250\n",
      "C2: KorzyÅ›ci                                      0.6616     0.2649     0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C3: ZagroÅ¼enia                                    0.6768     0.3205     0.2294\n",
      "C4: JakoÅ›Ä‡ dowodÃ³w                                0.6494     0.5267     0.5000\n",
      "C5: Choroba                                       0.8293     0.9067     1.0000\n",
      "C6: Å¹rÃ³dÅ‚a                                        0.6402     0.6878     0.7558\n",
      "C7: Alternatywy                                   0.5518     0.5017     0.4540\n",
      "C8: DostÄ™pnoÅ›Ä‡                                    0.6921     0.8061     0.9502\n",
      "C9: Innowacja                                     0.6646     0.7932     0.9769\n",
      "C10: Konflikty                                    0.8506     0.9182     1.0000\n",
      "================================================================================\n",
      "ÅšREDNIA (wszystkie kryteria)                      0.7034     0.6309     0.6575\n",
      "================================================================================\n",
      "\n",
      "Wyniki zapisane do: results_CNN_MultiOutput.json\n",
      "Model zapisany do: CNN_MultiOutput_model.h5\n",
      "Tokenizer zapisany do: tokenizer.pkl\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. WCZYTANIE DANYCH\n",
    "# --------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"HealthStory_cleaned.csv\")\n",
    "print(\"Wczytano plik HealthStory_cleaned.csv\")\n",
    "print(f\"Liczba rekordÃ³w: {len(df)}\")\n",
    "print(f\"Kolumny: {list(df.columns)}\")\n",
    "\n",
    "criteria_cols = [f\"C{i}\" for i in range(1, 11)]\n",
    "missing_cols = [c for c in criteria_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Brakuje kolumn: {missing_cols}\")\n",
    "\n",
    "# Dane wejÅ›ciowe i wyjÅ›ciowe\n",
    "X_texts = df[\"text\"].astype(str).tolist()\n",
    "y_criteria = df[criteria_cols].values.astype(int)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. PODZIAÅ NA TRAIN/TEST\n",
    "# --------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_texts, y_criteria, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Trening: {len(X_train)} prÃ³bek\")\n",
    "print(f\"Test: {len(X_test)} prÃ³bek\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. TOKENIZACJA BEZ OGRANICZEÅƒ\n",
    "# --------------------------------------------------------------------------\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(len(seq) for seq in tokenizer.texts_to_sequences(X_train))\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding='post')\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. MODEL CNN MULTI-TASK\n",
    "# --------------------------------------------------------------------------\n",
    "EMBEDDING_DIM = 300  \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='sigmoid')  # 10 wyjÅ›Ä‡ = 10 kryteriÃ³w\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. TRENING\n",
    "# --------------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6. PREDYKCJE\n",
    "# --------------------------------------------------------------------------\n",
    "y_pred_test = (model.predict(X_test_seq) > 0.5).astype(int)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 7. OCENA DLA KAÅ»DEGO KRYTERIUM\n",
    "# --------------------------------------------------------------------------\n",
    "criteria_names = [\n",
    "    \"C1: Koszty interwencji\", \"C2: KorzyÅ›ci\", \"C3: ZagroÅ¼enia\",\n",
    "    \"C4: JakoÅ›Ä‡ dowodÃ³w\", \"C5: Choroba\", \"C6: Å¹rÃ³dÅ‚a\",\n",
    "    \"C7: Alternatywy\", \"C8: DostÄ™pnoÅ›Ä‡\", \"C9: Innowacja\", \"C10: Konflikty\"\n",
    "]\n",
    "\n",
    "print(\"\\n WYNIKI DLA KAÅ»DEGO KRYTERIUM:\\n\")\n",
    "print(f\"{'Kryterium':<45} {'Accuracy':>10} {'F1':>10} {'Recall':>10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for i, name in enumerate(criteria_names):\n",
    "    y_true = y_test[:, i]\n",
    "    y_pred = y_pred_test[:, i]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    results.append((acc, f1, rec))\n",
    "    print(f\"{name:<45} {acc:>10.4f} {f1:>10.4f} {rec:>10.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 8. ÅšREDNIE WYNIKI\n",
    "# --------------------------------------------------------------------------\n",
    "avg_acc = np.mean([r[0] for r in results])\n",
    "avg_f1 = np.mean([r[1] for r in results])\n",
    "avg_rec = np.mean([r[2] for r in results])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"{'ÅšREDNIA (wszystkie kryteria)':<45} {avg_acc:>10.4f} {avg_f1:>10.4f} {avg_rec:>10.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 9. Zapis wynikÃ³w do pliku JSON\n",
    "# --------------------------------------------------------------------------\n",
    "results_summary = {\n",
    "    'dataset': 'HealthStory',\n",
    "    'n_samples': len(df),\n",
    "    'vocab_size': len(tokenizer.word_index),\n",
    "    'model': 'CNN Multi-Output',\n",
    "    'parameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'metrics_per_criterion': [],\n",
    "    'average_metrics': {\n",
    "        'accuracy': float(avg_acc),\n",
    "        'f1': float(avg_f1),\n",
    "        'recall': float(avg_rec)\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, name in enumerate(criteria_names):\n",
    "    results_summary['metrics_per_criterion'].append({\n",
    "        'criterion': name,\n",
    "        'accuracy': float(results[i][0]),\n",
    "        'f1': float(results[i][1]),\n",
    "        'recall': float(results[i][2])\n",
    "    })\n",
    "\n",
    "with open('results_CNN_MultiOutput.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nWyniki zapisane do: results_CNN_MultiOutput.json\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 10. ZAPIS MODELU I TOKENIZERA\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "# Zapis modelu Keras\n",
    "model.save(\"CNN_MultiOutput_model.h5\")\n",
    "print(\"Model zapisany do: CNN_MultiOutput_model.h5\")\n",
    "\n",
    "# Zapis tokenizer\n",
    "with open(\"tokenizer_MultiOutput.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer zapisany do: tokenizer.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be13341edc7cae59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T23:08:11.928223Z",
     "start_time": "2025-11-10T23:04:19.444509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN/TEST SPLIT\n",
      "Train: 1291  Test: 323\n",
      "Train label counts: [366 925]\n",
      "Test  label counts: [ 91 232]\n",
      "============================================================\n",
      "ğŸ“ Maksymalna dÅ‚ugoÅ›Ä‡ sekwencji w danych: 2238\n",
      "ğŸ”¹ Wczytywanie FastText embeddings (to moÅ¼e chwilÄ™ potrwaÄ‡)...\n",
      "âœ… Znaleziono embeddingi dla 17,008 / 19,818 sÅ‚Ã³w.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\NQW483\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           â”‚ ?                      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ ?                      â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ ?                      â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           â”‚ ?                      â”‚     \u001b[38;5;34m5,945,400\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ ?                      â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ ?                      â”‚   \u001b[38;5;34m0\u001b[0m (unbuilt) â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> (22.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,945,400\u001b[0m (22.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,945,400</span> (22.68 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,945,400\u001b[0m (22.68 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2514s\u001b[0m 122s/step - accuracy: 0.6971 - loss: 0.6307 - val_accuracy: 0.7183 - val_loss: 0.6074\n",
      "Epoch 2/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2840s\u001b[0m 136s/step - accuracy: 0.7165 - loss: 0.6003 - val_accuracy: 0.7183 - val_loss: 0.5971\n",
      "Epoch 3/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4087s\u001b[0m 199s/step - accuracy: 0.7165 - loss: 0.5855 - val_accuracy: 0.7183 - val_loss: 0.5994\n",
      "Epoch 4/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3128s\u001b[0m 149s/step - accuracy: 0.7173 - loss: 0.5955 - val_accuracy: 0.7183 - val_loss: 0.5951\n",
      "Epoch 5/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4489s\u001b[0m 218s/step - accuracy: 0.7180 - loss: 0.5781 - val_accuracy: 0.7152 - val_loss: 0.5982\n",
      "Epoch 6/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3582s\u001b[0m 171s/step - accuracy: 0.7173 - loss: 0.5679 - val_accuracy: 0.7183 - val_loss: 0.5925\n",
      "Epoch 7/7\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3755s\u001b[0m 179s/step - accuracy: 0.7258 - loss: 0.5670 - val_accuracy: 0.7121 - val_loss: 0.5938\n",
      "\u001b[1m41/41\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2662s\u001b[0m 66s/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m177s\u001b[0m 16s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š WYNIKI MODELU LSTM\n",
      "Train Accuracy: 0.7328\n",
      "Test  Accuracy: 0.7121\n",
      "Precision: 0.7192\n",
      "Recall:    0.9828\n",
      "F1-Score:  0.8306\n",
      "ROC-AUC:   0.5024\n",
      "============================================================\n",
      "âœ… Model zapisany jako lstm_fasttext_full_unlimited.h5\n",
      "\n",
      "Wyniki zapisane do: results_LSTM_Embedding.json\n",
      "Analiza zakoÅ„czona pomyÅ›lnie!\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# --- PARAMETRY ---\n",
    "EMBEDDING_DIM = 300\n",
    "FASTTEXT_PATH = 'cc.en.300.vec'\n",
    "\n",
    "# --- 1. Wczytanie danych ---\n",
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "df_clean = df_clean.dropna(subset=['text'])\n",
    "df_clean = df_clean[df_clean['text'].str.strip() != \"\"]\n",
    "\n",
    "texts = df_clean['text'].astype(str).tolist()\n",
    "y = (df_clean['rating'] >= 3).astype(int).values\n",
    "\n",
    "# --- 2. PodziaÅ‚ train/test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(f\"Train: {len(X_train)}  Test: {len(X_test)}\")\n",
    "print(\"Train label counts:\", np.bincount(y_train))\n",
    "print(\"Test  label counts:\", np.bincount(y_test))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 3. Tokenizacja bez ograniczeÅ„ ---\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(len(seq) for seq in X_train_seq + X_test_seq)\n",
    "print(f\"ğŸ“ Maksymalna dÅ‚ugoÅ›Ä‡ sekwencji w danych: {max_length}\")\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "X_train_seq = np.array(X_train_seq, dtype=np.int32)\n",
    "X_test_seq = np.array(X_test_seq, dtype=np.int32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "# --- 4. Embeddingi FastText ---\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "if os.path.exists(FASTTEXT_PATH):\n",
    "    print(\"ğŸ”¹ Wczytywanie FastText embeddings (to moÅ¼e chwilÄ™ potrwaÄ‡)...\")\n",
    "    embedding_index = {}\n",
    "    with open(FASTTEXT_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = f.readline().strip().split()\n",
    "        if len(first_line) != 2 or not all(x.isdigit() for x in first_line):\n",
    "            f.seek(0)\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) < EMBEDDING_DIM + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:EMBEDDING_DIM + 1], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_index:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "            found += 1\n",
    "    print(f\"âœ… Znaleziono embeddingi dla {found:,} / {num_words:,} sÅ‚Ã³w.\")\n",
    "else:\n",
    "    print(\"âš ï¸ Brak pliku FastText â€” losowa inicjalizacja embeddingÃ³w.\")\n",
    "    embedding_matrix = np.random.normal(size=(num_words, EMBEDDING_DIM)).astype('float32')\n",
    "\n",
    "# --- 5. Model LSTM ---\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=False\n",
    "    ),\n",
    "    Bidirectional(LSTM(128, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 6. Trening ---\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=7,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 7. Ewaluacja ---\n",
    "y_train_pred = (model.predict(X_train_seq) > 0.5).astype(int).ravel()\n",
    "y_test_pred = (model.predict(X_test_seq) > 0.5).astype(int).ravel()\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "rec_test = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "f1_test = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ğŸ“Š WYNIKI MODELU LSTM\")\n",
    "print(f\"Train Accuracy: {acc_train:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc_test:.4f}\")\n",
    "print(f\"Precision: {prec_test:.4f}\")\n",
    "print(f\"Recall:    {rec_test:.4f}\")\n",
    "print(f\"F1-Score:  {f1_test:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc_test:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 8. Zapis modelu ---\n",
    "model.save(\"lstm_fasttext_full_unlimited.h5\")\n",
    "print(\"âœ… Model zapisany jako lstm_fasttext_full_unlimited.h5\")\n",
    "\n",
    "# --- 14. Zapisz wyniki do pliku JSON ---\n",
    "results_summary = {\n",
    "    'dataset': 'HealthStory',\n",
    "    'n_samples': len(df_clean),\n",
    "    'n_features': len(tokenizer.word_index),\n",
    "    'model': 'BiLSTM + FastText embeddings',\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'accuracy': float(acc_train)\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': float(acc_test),\n",
    "            'precision': float(prec_test),\n",
    "            'recall': float(rec_test),\n",
    "            'f1': float(f1_test),\n",
    "            'roc_auc': float(auc_test)\n",
    "        },\n",
    "        'history': {\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'val_accuracy': history.history['val_accuracy']\n",
    "        }\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'trainable_embeddings': False,\n",
    "        'lstm_units': 128,\n",
    "        'bidirectional': True\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = 'results_LSTM_Embedding.json'\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "with open(\"results_LSTM_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "print(f\"\\nWyniki zapisane do: {json_path}\")\n",
    "print(\"Analiza zakoÅ„czona pomyÅ›lnie!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
