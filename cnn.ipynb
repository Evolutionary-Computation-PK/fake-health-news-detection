{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:09:43.624722Z",
     "start_time": "2025-11-10T01:09:27.725148Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle"
   ],
   "id": "997c9b0a62d1223f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:09:53.994787Z",
     "start_time": "2025-11-10T01:09:43.654954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Wczytanie JSON i ekstrakcja kryteri√≥w\n",
    "# --------------------------------------------------------------------------\n",
    "\n",
    "def extract_from_healthnews_json(json_path):\n",
    "    \"\"\"\n",
    "    Wczytuje dane z pliku JSON (lista rekord√≥w lub pojedynczy rekord)\n",
    "    i zwraca DataFrame z tekstem, ratingiem i 10 kryteriami (Satisfactory = 1, Not = 0).\n",
    "    \"\"\"\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    if isinstance(data, dict):\n",
    "        data = [data]\n",
    "    \n",
    "    records = []\n",
    "    for article in data:\n",
    "        record = {\n",
    "            'news_id': article.get('news_id'),\n",
    "            'title': article.get('title'),\n",
    "            'text': article.get('text'),\n",
    "            'rating': article.get('rating'),\n",
    "        }\n",
    "\n",
    "        # Ekstrakcja 10 kryteri√≥w\n",
    "        criteria = article.get('criteria', [])\n",
    "        for i, crit in enumerate(criteria, start=1):\n",
    "            answer = crit.get('answer', '').strip()\n",
    "            if answer.lower().startswith('satisfactory'):\n",
    "                record[f'C{i}'] = 1\n",
    "            elif answer.lower().startswith('not'):\n",
    "                record[f'C{i}'] = 0\n",
    "            else:\n",
    "                record[f'C{i}'] = 1  # \"Not Applicable\" ‚Üí 1\n",
    "        \n",
    "        # BrakujƒÖce kryteria uzupe≈Çniamy jako Satisfactory\n",
    "        for j in range(len(criteria) + 1, 11):\n",
    "            record[f'C{j}'] = 1\n",
    "\n",
    "        records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Wczytanie danych\n",
    "# --------------------------------------------------------------------------\n",
    "df = extract_from_healthnews_json(\"HealthStory_combined.json\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"Wczytano dane z JSON\")\n",
    "print(f\"Liczba rekord√≥w: {len(df)}\")\n",
    "print(f\"Kolumny: {list(df.columns)}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Przygotowanie labels\n",
    "# --------------------------------------------------------------------------\n",
    "df['rating'] = pd.to_numeric(df['rating'], errors='coerce')\n",
    "df = df.dropna(subset=['text', 'rating']).reset_index(drop=True)\n",
    "\n",
    "# Binary label\n",
    "df['binary_label'] = (df['rating'] >= 3).astype(int)\n",
    "\n",
    "print(\"PODSUMOWANIE LABELI\")\n",
    "print(f\"RATING: {df['rating'].shape}  (min={df['rating'].min()}, max={df['rating'].max()})\")\n",
    "print(f\"BINARY: {df['binary_label'].shape}  ‚Üí {df['binary_label'].sum()} reliable, {(df['binary_label']==0).sum()} fake\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Czyszczenie tekst√≥w\n",
    "# --------------------------------------------------------------------------\n",
    "nltk.download('stopwords')\n",
    "english_stopwords = set(stopwords.words('english'))\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"[^a-z ]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([w for w in text.split() if w not in english_stopwords])\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "print(\"Czyszczenie tekst√≥w (to mo≈ºe potrwaƒá kilka minut)...\")\n",
    "df['text'] = df['text'].astype(str)\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "df['text'] = df['text'].apply(remove_stopwords)\n",
    "df['text'] = df['text'].apply(lemmatize_text)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# Zapis do pliku\n",
    "# --------------------------------------------------------------------------\n",
    "cleaned_path = \"HealthStory_cleaned.csv\"\n",
    "df.to_csv(cleaned_path, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"Zapisano wszystko do jednego pliku: {cleaned_path}\")\n",
    "print(f\" Kolumny: {list(df.columns)}\")\n",
    "print(f\" Wiersze: {len(df)}\")\n",
    "print(\"=\"*80)\n"
   ],
   "id": "7e9f5e15a32e3072",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Wczytano dane z JSON\n",
      "Liczba rekord√≥w: 1638\n",
      "Kolumny: ['news_id', 'title', 'text', 'rating', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10']\n",
      "================================================================================\n",
      "PODSUMOWANIE LABELI\n",
      "RATING: (1638,)  (min=0, max=5)\n",
      "BINARY: (1638,)  ‚Üí 1178 reliable, 460 fake\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czyszczenie tekst√≥w (to mo≈ºe potrwaƒá kilka minut)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 97\u001B[0m\n\u001B[0;32m     95\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(clean_text)\n\u001B[0;32m     96\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(remove_stopwords)\n\u001B[1;32m---> 97\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlemmatize_text\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     99\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# Zapis do pliku\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# --------------------------------------------------------------------------\u001B[39;00m\n\u001B[0;32m    102\u001B[0m cleaned_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHealthStory_cleaned.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\pandas\\core\\series.py:4630\u001B[0m, in \u001B[0;36mSeries.apply\u001B[1;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[0;32m   4520\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[0;32m   4521\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   4522\u001B[0m     func: AggFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4525\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m   4526\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[0;32m   4527\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4528\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[0;32m   4529\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4628\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[0;32m   4629\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4630\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[1;32m-> 1025\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1074\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1075\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[1;32m-> 1076\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m            \u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m            \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m            \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1082\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[0;32m   1083\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[0;32m   1084\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[0;32m   1085\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[1;34m()\u001B[0m\n",
      "Cell \u001B[1;32mIn[2], line 90\u001B[0m, in \u001B[0;36mlemmatize_text\u001B[1;34m(text)\u001B[0m\n\u001B[0;32m     89\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlemmatize_text\u001B[39m(text):\n\u001B[1;32m---> 90\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin([token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc])\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\spacy\\language.py:1049\u001B[0m, in \u001B[0;36mLanguage.__call__\u001B[1;34m(self, text, disable, component_cfg)\u001B[0m\n\u001B[0;32m   1047\u001B[0m     error_handler \u001B[38;5;241m=\u001B[39m proc\u001B[38;5;241m.\u001B[39mget_error_handler()\n\u001B[0;32m   1048\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1049\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mproc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcomponent_cfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m   1050\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1051\u001B[0m     \u001B[38;5;66;03m# This typically happens if a component is not initialized\u001B[39;00m\n\u001B[0;32m   1052\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE109\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname)) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001B[0m, in \u001B[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\spacy\\pipeline\\tok2vec.py:126\u001B[0m, in \u001B[0;36mTok2Vec.predict\u001B[1;34m(self, docs)\u001B[0m\n\u001B[0;32m    124\u001B[0m     width \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mget_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnO\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39malloc((\u001B[38;5;241m0\u001B[39m, width)) \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[1;32m--> 126\u001B[0m tokvecs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m tokvecs\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:334\u001B[0m, in \u001B[0;36mModel.predict\u001B[1;34m(self, X)\u001B[0m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m OutT:\n\u001B[0;32m    331\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001B[39;00m\n\u001B[0;32m    332\u001B[0m \u001B[38;5;124;03m    only the output, instead of the `(output, callback)` tuple.\u001B[39;00m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 334\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\with_array.py:42\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, Xseq, is_train)\u001B[0m\n\u001B[0;32m     40\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers[\u001B[38;5;241m0\u001B[39m](Xseq, is_train)\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 42\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(Tuple[SeqT, Callable], \u001B[43m_list_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mXseq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\with_array.py:77\u001B[0m, in \u001B[0;36m_list_forward\u001B[1;34m(model, Xs, is_train)\u001B[0m\n\u001B[0;32m     75\u001B[0m lengths \u001B[38;5;241m=\u001B[39m NUMPY_OPS\u001B[38;5;241m.\u001B[39masarray1i([\u001B[38;5;28mlen\u001B[39m(seq) \u001B[38;5;28;01mfor\u001B[39;00m seq \u001B[38;5;129;01min\u001B[39;00m Xs])\n\u001B[0;32m     76\u001B[0m Xf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(Xs, pad\u001B[38;5;241m=\u001B[39mpad)\n\u001B[1;32m---> 77\u001B[0m Yf, get_dXf \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mXf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbackprop\u001B[39m(dYs: ListXd) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ListXd:\n\u001B[0;32m     80\u001B[0m     dYf \u001B[38;5;241m=\u001B[39m layer\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mflatten(dYs, pad\u001B[38;5;241m=\u001B[39mpad)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\residual.py:41\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     39\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m d_output \u001B[38;5;241m+\u001B[39m dX\n\u001B[1;32m---> 41\u001B[0m Y, backprop_layer \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(X, \u001B[38;5;28mlist\u001B[39m):\n\u001B[0;32m     43\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [X[i] \u001B[38;5;241m+\u001B[39m Y[i] \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(X))], backprop\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "    \u001B[1;31m[... skipping similar frames: Model.__call__ at line 310 (1 times)]\u001B[0m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     52\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m layer \u001B[38;5;129;01min\u001B[39;00m model\u001B[38;5;241m.\u001B[39mlayers:\n\u001B[1;32m---> 54\u001B[0m     Y, inc_layer_grad \u001B[38;5;241m=\u001B[39m \u001B[43mlayer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     55\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mappend(inc_layer_grad)\n\u001B[0;32m     56\u001B[0m     X \u001B[38;5;241m=\u001B[39m Y\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\model.py:310\u001B[0m, in \u001B[0;36mModel.__call__\u001B[1;34m(self, X, is_train)\u001B[0m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, X: InT, is_train: \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[OutT, Callable]:\n\u001B[0;32m    308\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Call the model's `forward` function, returning the output and a\u001B[39;00m\n\u001B[0;32m    309\u001B[0m \u001B[38;5;124;03m    callback to compute the gradients via backpropagation.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 310\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mis_train\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_train\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\thinc\\layers\\maxout.py:52\u001B[0m, in \u001B[0;36mforward\u001B[1;34m(model, X, is_train)\u001B[0m\n\u001B[0;32m     50\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mget_param(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mW\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     51\u001B[0m W \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape2f(W, nO \u001B[38;5;241m*\u001B[39m nP, nI)\n\u001B[1;32m---> 52\u001B[0m Y \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgemm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mW\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrans2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     53\u001B[0m Y \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape1f(b, nO \u001B[38;5;241m*\u001B[39m nP)\n\u001B[0;32m     54\u001B[0m Z \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mreshape3f(Y, Y\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], nO, nP)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T22:54:08.234490Z",
     "start_time": "2025-11-09T22:54:08.083999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "y = (df_clean['rating'] >= 3).astype(int).values\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "count_dict = dict(zip(unique, counts))\n",
    "print(\"Liczba 0 i 1:\", count_dict)\n"
   ],
   "id": "e9769d58f48cf9f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba 0 i 1: {0: 460, 1: 1178}\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-10T01:20:01.554982Z",
     "start_time": "2025-11-10T01:09:55.055048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- PARAMETRY ---\n",
    "EMBEDDING_DIM = 300\n",
    "FASTTEXT_PATH = 'cc.en.300.vec'\n",
    "\n",
    "# --- 1. Wczytanie danych ---\n",
    "df_clean = pd.read_csv('HealthStory_cleaned.csv')\n",
    "\n",
    "df_clean = df_clean.dropna(subset=['text'])\n",
    "df_clean = df_clean[df_clean['text'].str.strip() != \"\"]\n",
    "\n",
    "texts = df_clean['text'].astype(str).tolist()\n",
    "y = (df_clean['rating'] >= 3).astype(int).values\n",
    "\n",
    "# --- 2. Podzia≈Ç train/test ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(f\"Train: {len(X_train)}  Test: {len(X_test)}\")\n",
    "print(\"Train label counts:\", np.bincount(y_train))\n",
    "print(\"Test  label counts:\", np.bincount(y_test))\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 3. Tokenizacja bez ogranicze≈Ñ ---\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_length = max(len(seq) for seq in X_train_seq + X_test_seq)\n",
    "print(f\"üìè Maksymalna d≈Çugo≈õƒá sekwencji w danych: {max_length}\")\n",
    "\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "X_train_seq = np.array(X_train_seq, dtype=np.int32)\n",
    "X_test_seq = np.array(X_test_seq, dtype=np.int32)\n",
    "y_train = np.array(y_train, dtype=np.int32)\n",
    "y_test = np.array(y_test, dtype=np.int32)\n",
    "\n",
    "# --- 4. Embeddingi FastText ---\n",
    "word_index = tokenizer.word_index\n",
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "if os.path.exists(FASTTEXT_PATH):\n",
    "    print(\"üîπ Wczytywanie FastText embeddings (to mo≈ºe chwilƒô potrwaƒá)...\")\n",
    "    embedding_index = {}\n",
    "    with open(FASTTEXT_PATH, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        first_line = f.readline().strip().split()\n",
    "        if len(first_line) != 2 or not all(x.isdigit() for x in first_line):\n",
    "            f.seek(0)\n",
    "        for line in f:\n",
    "            values = line.rstrip().split(' ')\n",
    "            if len(values) < EMBEDDING_DIM + 1:\n",
    "                continue\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:EMBEDDING_DIM + 1], dtype='float32')\n",
    "            embedding_index[word] = coefs\n",
    "\n",
    "    found = 0\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_index:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "            found += 1\n",
    "    print(f\"‚úÖ Znaleziono embeddingi dla {found:,} / {num_words:,} s≈Ç√≥w.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Brak pliku FastText ‚Äî losowa inicjalizacja embedding√≥w.\")\n",
    "    embedding_matrix = np.random.normal(size=(num_words, EMBEDDING_DIM)).astype('float32')\n",
    "\n",
    "# --- 5. Model CNN ---\n",
    "model = Sequential([\n",
    "    Embedding(\n",
    "        input_dim=num_words,\n",
    "        output_dim=EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=False\n",
    "    ),\n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --- 6. Trening ---\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- 7. Ewaluacja ---\n",
    "y_train_pred = (model.predict(X_train_seq) > 0.5).astype(int).ravel()\n",
    "y_test_pred = (model.predict(X_test_seq) > 0.5).astype(int).ravel()\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred, zero_division=0)\n",
    "rec_test = recall_score(y_test, y_test_pred, zero_division=0)\n",
    "f1_test = f1_score(y_test, y_test_pred, zero_division=0)\n",
    "auc_test = roc_auc_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"üìä WYNIKI MODELU CNN\")\n",
    "print(f\"Train Accuracy: {acc_train:.4f}\")\n",
    "print(f\"Test  Accuracy: {acc_test:.4f}\")\n",
    "print(f\"Precision: {prec_test:.4f}\")\n",
    "print(f\"Recall:    {rec_test:.4f}\")\n",
    "print(f\"F1-Score:  {f1_test:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc_test:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- 8. Zapis modelu ---\n",
    "model.save(\"cnn_fasttext_full_unlimited.h5\")\n",
    "print(\"‚úÖ Model zapisany jako cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "# --- 14. Zapisz wyniki do pliku JSON ---\n",
    "results_summary = {\n",
    "    'dataset': 'HealthStory',\n",
    "    'n_samples': len(df_clean),\n",
    "    'n_features': len(tokenizer.word_index),\n",
    "    'model': 'CNN + FastText embeddings',\n",
    "    'metrics': {\n",
    "        'train': {\n",
    "            'accuracy': float(acc_train)\n",
    "        },\n",
    "        'test': {\n",
    "            'accuracy': float(acc_test),\n",
    "            'precision': float(prec_test),\n",
    "            'recall': float(rec_test),\n",
    "            'f1': float(f1_test),\n",
    "            'roc_auc': float(auc_test)\n",
    "        },\n",
    "        'history': {\n",
    "            'loss': history.history['loss'],\n",
    "            'val_loss': history.history['val_loss'],\n",
    "            'accuracy': history.history['accuracy'],\n",
    "            'val_accuracy': history.history['val_accuracy']\n",
    "        }\n",
    "    },\n",
    "    'parameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64,\n",
    "        'trainable_embeddings': False\n",
    "    }\n",
    "}\n",
    "\n",
    "json_path = 'results_CNN_Embedding.json'\n",
    "with open(json_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "with open(\"results_CNN_tokenizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "    \n",
    "print(f\"\\nWyniki zapisane do: {json_path}\")\n",
    "print(\"Analiza zako≈Ñczona pomy≈õlnie!\")\n"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAIN/TEST SPLIT\n",
      "Train: 1291  Test: 323\n",
      "Train label counts: [366 925]\n",
      "Test  label counts: [ 91 232]\n",
      "============================================================\n",
      "üìè Maksymalna d≈Çugo≈õƒá sekwencji w danych: 2238\n",
      "‚ö†Ô∏è Brak pliku FastText ‚Äî losowa inicjalizacja embedding√≥w.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 2238, 300)         5950500   \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 2234, 128)         192128    \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,150,949\n",
      "Trainable params: 200,449\n",
      "Non-trainable params: 5,950,500\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "21/21 [==============================] - 70s 3s/step - loss: 1.1724 - accuracy: 0.6189 - val_loss: 0.5884 - val_accuracy: 0.7090\n",
      "Epoch 2/10\n",
      "21/21 [==============================] - 63s 3s/step - loss: 0.5460 - accuracy: 0.7483 - val_loss: 0.5883 - val_accuracy: 0.7121\n",
      "Epoch 3/10\n",
      "21/21 [==============================] - 60s 3s/step - loss: 0.3970 - accuracy: 0.8226 - val_loss: 0.5951 - val_accuracy: 0.7183\n",
      "Epoch 4/10\n",
      "21/21 [==============================] - 62s 3s/step - loss: 0.2899 - accuracy: 0.8923 - val_loss: 0.6318 - val_accuracy: 0.7214\n",
      "Epoch 5/10\n",
      "21/21 [==============================] - 56s 3s/step - loss: 0.2116 - accuracy: 0.9233 - val_loss: 0.5931 - val_accuracy: 0.7059\n",
      "Epoch 6/10\n",
      "21/21 [==============================] - 61s 3s/step - loss: 0.1698 - accuracy: 0.9527 - val_loss: 0.5848 - val_accuracy: 0.7028\n",
      "Epoch 7/10\n",
      "21/21 [==============================] - 55s 3s/step - loss: 0.1363 - accuracy: 0.9636 - val_loss: 0.6090 - val_accuracy: 0.7090\n",
      "Epoch 8/10\n",
      "21/21 [==============================] - 52s 2s/step - loss: 0.1132 - accuracy: 0.9690 - val_loss: 0.6480 - val_accuracy: 0.7152\n",
      "Epoch 9/10\n",
      "21/21 [==============================] - 50s 2s/step - loss: 0.0932 - accuracy: 0.9737 - val_loss: 0.6436 - val_accuracy: 0.7214\n",
      "Epoch 10/10\n",
      "21/21 [==============================] - 49s 2s/step - loss: 0.0804 - accuracy: 0.9806 - val_loss: 0.7493 - val_accuracy: 0.7152\n",
      "41/41 [==============================] - 21s 504ms/step\n",
      "11/11 [==============================] - 5s 466ms/step\n",
      "============================================================\n",
      "üìä WYNIKI MODELU CNN\n",
      "Train Accuracy: 0.9892\n",
      "Test  Accuracy: 0.7152\n",
      "Precision: 0.7244\n",
      "Recall:    0.9741\n",
      "F1-Score:  0.8309\n",
      "ROC-AUC:   0.5145\n",
      "============================================================\n",
      "‚úÖ Model zapisany jako cnn_fasttext_full_unlimited.h5\n",
      "\n",
      "Wyniki zapisane do: results_CNN_Embedding.json\n",
      "Analiza zako≈Ñczona pomy≈õlnie!\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-10T01:36:46.437707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_loaded = load_model(\"cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "background = np.array(X_train_seq[:100], dtype=np.float32)\n",
    "test_data = np.array(X_test_seq[:10], dtype=np.float32)\n",
    "\n",
    "explainer = shap.Explainer(model_loaded, background, algorithm=\"partition\")\n",
    "\n",
    "shap_values = explainer(test_data)\n",
    "\n",
    "# Wykres s≈Çupkowy wp≈Çywu cech/token√≥w\n",
    "shap.plots.bar(shap_values)\n"
   ],
   "id": "96fe9658253db83e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|‚ñà‚ñà‚ñé       | 118/498 [03:48<12:22,  1.96s/it]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:33:52.301079Z",
     "start_time": "2025-11-10T01:33:51.693472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import shap\n",
    "import json\n",
    "\n",
    "# Wczytaj tokenizer\n",
    "with open(\"results_CNN_tokenizer.pkl\", \"rb\") as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "# Odwr√≥cony s≈Çownik token -> s≈Çowo\n",
    "reverse_word_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    return ' '.join([reverse_word_map.get(i, '') for i in seq if i != 0])\n",
    "\n",
    "# Przygotowanie przyk≈Çadowych tekst√≥w\n",
    "sample_texts = [seq_to_text(seq) for seq in X_test_seq[:5]]\n",
    "\n",
    "# Za≈Çaduj model\n",
    "model_loaded = load_model(\"cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "# Konwersja danych do float32\n",
    "background = np.array(X_train_seq[:200], dtype=np.float32)\n",
    "test_data = np.array(X_test_seq[:5], dtype=np.float32)\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.Explainer(model_loaded, background, algorithm=\"partition\")\n",
    "shap_values = explainer(test_data)\n",
    "\n",
    "# Tworzenie JSON z warto≈õciami SHAP dla token√≥w\n",
    "shap_values_dict = []\n",
    "for i, sv in enumerate(shap_values.values):\n",
    "    text_tokens = X_test_seq[i]\n",
    "    token_shap = [\n",
    "        {\"word\": reverse_word_map.get(int(token), \"\"), \"shap_value\": float(value)}\n",
    "        for token, value in zip(text_tokens, sv)\n",
    "        if token != 0\n",
    "    ]\n",
    "    shap_values_dict.append({\n",
    "        \"text_index\": i,\n",
    "        \"original_text\": sample_texts[i],\n",
    "        \"shap_per_word\": token_shap\n",
    "    })\n",
    "\n",
    "# Zapis do JSON\n",
    "with open(\"cnn_fasttext_shap_values.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(shap_values_dict, f, ensure_ascii=False, indent=2)\n"
   ],
   "id": "1ef1249cb5855843",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 17\u001B[0m\n\u001B[0;32m     13\u001B[0m model_loaded \u001B[38;5;241m=\u001B[39m load_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcnn_fasttext_full_unlimited.h5\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     16\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model_loaded, X_train_seq[:\u001B[38;5;241m200\u001B[39m])\n\u001B[1;32m---> 17\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_seq\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m shap_values_dict \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, sv \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(shap_values\u001B[38;5;241m.\u001B[39mvalues):\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:76\u001B[0m, in \u001B[0;36mPermutation.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, max_evals\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, main_effects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, error_bounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     73\u001B[0m              outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     74\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_explainer.py:264\u001B[0m, in \u001B[0;36mExplainer.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     feature_names \u001B[38;5;241m=\u001B[39m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(args))]\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row_args \u001B[38;5;129;01min\u001B[39;00m show_progress(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39margs), num_rows, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m explainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, silent):\n\u001B[1;32m--> 264\u001B[0m     row_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexplain_row\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrow_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     values\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    269\u001B[0m     output_indices\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_indices\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:158\u001B[0m, in \u001B[0;36mPermutation.explain_row\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001B[0m\n\u001B[0;32m    155\u001B[0m     history_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m npermutations \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_evals=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_evals\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m2\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(inds)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    160\u001B[0m expected_value \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    162\u001B[0m \u001B[38;5;66;03m# compute the main effects if we need to\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:33:56.707678Z",
     "start_time": "2025-11-10T01:33:56.225441Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import shap\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_loaded = load_model(\"cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "reverse_word_map = {v: k for k, v in tokenizer.word_index.items()}\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    return ' '.join([reverse_word_map.get(i, '') for i in seq if i != 0])\n",
    "\n",
    "# Przygotowanie danych\n",
    "background = np.array(X_train_seq[:200], dtype=np.float32)\n",
    "test_data = np.array(X_test_seq[:5], dtype=np.float32)\n",
    "sample_texts = [seq_to_text(seq) for seq in X_test_seq[:5]]\n",
    "\n",
    "# SHAP Explainer\n",
    "explainer = shap.Explainer(model_loaded, background, algorithm=\"partition\")\n",
    "shap_values = explainer(test_data)\n",
    "\n",
    "# --- Zapis per token do JSON ---\n",
    "shap_values_dict = []\n",
    "for i, sv in enumerate(shap_values.values):\n",
    "    text_tokens = X_test_seq[i]\n",
    "    token_shap = [\n",
    "        {\"word\": reverse_word_map.get(int(token), \"\"), \"shap_value\": float(value)}\n",
    "        for token, value in zip(text_tokens, sv)\n",
    "        if token != 0\n",
    "    ]\n",
    "    shap_values_dict.append({\n",
    "        \"text_index\": i,\n",
    "        \"original_text\": sample_texts[i],\n",
    "        \"shap_per_word\": token_shap\n",
    "    })\n",
    "\n",
    "with open(\"cnn_fasttext_shap_values.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(shap_values_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"SHAP values zapisane do cnn_fasttext_shap_values.json\")\n",
    "\n",
    "# --- Globalna wizualizacja wp≈Çywu token√≥w ---\n",
    "print(\"Tworzenie wykresu s≈Çupkowego globalnego wp≈Çywu token√≥w...\")\n",
    "shap.plots.bar(shap_values, max_display=50)\n",
    "\n",
    "# --- Interaktywna wizualizacja pierwszego tekstu (opcjonalnie) ---\n",
    "shap.plots.text(shap_values[0])\n"
   ],
   "id": "62256636773af407",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Explainer SHAP\u001B[39;00m\n\u001B[0;32m     17\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model_loaded, X_train_seq[:\u001B[38;5;241m200\u001B[39m])\n\u001B[1;32m---> 18\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_seq\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# --- Zapis per s≈Çowo do JSON ---\u001B[39;00m\n\u001B[0;32m     21\u001B[0m shap_values_dict \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:76\u001B[0m, in \u001B[0;36mPermutation.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, max_evals\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, main_effects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, error_bounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     73\u001B[0m              outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     74\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_explainer.py:264\u001B[0m, in \u001B[0;36mExplainer.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     feature_names \u001B[38;5;241m=\u001B[39m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(args))]\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row_args \u001B[38;5;129;01min\u001B[39;00m show_progress(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39margs), num_rows, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m explainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, silent):\n\u001B[1;32m--> 264\u001B[0m     row_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexplain_row\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrow_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     values\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    269\u001B[0m     output_indices\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_indices\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:158\u001B[0m, in \u001B[0;36mPermutation.explain_row\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001B[0m\n\u001B[0;32m    155\u001B[0m     history_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m npermutations \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_evals=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_evals\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m2\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(inds)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    160\u001B[0m expected_value \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    162\u001B[0m \u001B[38;5;66;03m# compute the main effects if we need to\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T01:34:02.459365Z",
     "start_time": "2025-11-10T01:34:01.916735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import shap\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_loaded = load_model(\"cnn_fasttext_full_unlimited.h5\")\n",
    "\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "def seq_to_text(seq):\n",
    "    return ' '.join([reverse_word_map.get(i, '') for i in seq if i != 0])\n",
    "\n",
    "# Przyk≈Çadowe teksty do analizy\n",
    "sample_texts = [seq_to_text(seq) for seq in X_test_seq[:5]]\n",
    "\n",
    "# Explainer SHAP\n",
    "explainer = shap.Explainer(model_loaded, X_train_seq[:200])\n",
    "shap_values = explainer(X_test_seq[:5])\n",
    "\n",
    "# ---  Zapis per s≈Çowo do JSON ---\n",
    "shap_values_dict = []\n",
    "for i, sv in enumerate(shap_values.values):\n",
    "    text_tokens = X_test_seq[i]\n",
    "    token_shap = [\n",
    "        {\"word\": reverse_word_map.get(int(token), \"\"), \"shap_value\": float(value)}\n",
    "        for token, value in zip(text_tokens, sv)\n",
    "        if token != 0\n",
    "    ]\n",
    "    shap_values_dict.append({\n",
    "        \"text_index\": i,\n",
    "        \"original_text\": sample_texts[i],\n",
    "        \"shap_per_word\": token_shap\n",
    "    })\n",
    "\n",
    "with open(\"cnn_fasttext_shap_values.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(shap_values_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "print(\"Wykres s≈Çupkowy globalnego wp≈Çywu token√≥w...\")\n",
    "shap.plots.bar(shap_values)\n"
   ],
   "id": "6e0a0adef944da27",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 18\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;66;03m# Explainer SHAP\u001B[39;00m\n\u001B[0;32m     17\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(model_loaded, X_train_seq[:\u001B[38;5;241m200\u001B[39m])\n\u001B[1;32m---> 18\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_test_seq\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# ---  Zapis per s≈Çowo do JSON ---\u001B[39;00m\n\u001B[0;32m     21\u001B[0m shap_values_dict \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:76\u001B[0m, in \u001B[0;36mPermutation.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, max_evals\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m, main_effects\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, error_bounds\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     73\u001B[0m              outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, silent\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m     74\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" Explain the output of the model on the given arguments.\u001B[39;00m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m     77\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\n\u001B[0;32m     79\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_explainer.py:264\u001B[0m, in \u001B[0;36mExplainer.__call__\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001B[0m\n\u001B[0;32m    262\u001B[0m     feature_names \u001B[38;5;241m=\u001B[39m [[] \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(args))]\n\u001B[0;32m    263\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m row_args \u001B[38;5;129;01min\u001B[39;00m show_progress(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39margs), num_rows, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m explainer\u001B[39m\u001B[38;5;124m\"\u001B[39m, silent):\n\u001B[1;32m--> 264\u001B[0m     row_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexplain_row\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mrow_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_evals\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_evals\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmain_effects\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmain_effects\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merror_bounds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merror_bounds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msilent\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msilent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    268\u001B[0m     values\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvalues\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    269\u001B[0m     output_indices\u001B[38;5;241m.\u001B[39mappend(row_result\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_indices\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\fake\\lib\\site-packages\\shap\\explainers\\_permutation.py:158\u001B[0m, in \u001B[0;36mPermutation.explain_row\u001B[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001B[0m\n\u001B[0;32m    155\u001B[0m     history_pos \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m npermutations \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 158\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_evals=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmax_evals\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;241m2\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mlen\u001B[39m(inds)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    160\u001B[0m expected_value \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    162\u001B[0m \u001B[38;5;66;03m# compute the main effects if we need to\u001B[39;00m\n",
      "\u001B[1;31mValueError\u001B[0m: max_evals=500 is too low for the Permutation explainer, it must be at least 2 * num_features + 1 = 3089!"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Wybierz pierwszy przyk≈Çad\n",
    "example_index = 0\n",
    "shap_example = shap_values.values[example_index]\n",
    "text_tokens = X_test_seq[example_index]\n",
    "\n",
    "# Utw√≥rz s≈Çownik {s≈Çowo: wp≈Çyw SHAP}\n",
    "token_shap_dict = {\n",
    "    reverse_word_map.get(int(token), ''): float(value)\n",
    "    for token, value in zip(text_tokens, shap_example)\n",
    "    if token != 0\n",
    "}\n",
    "\n",
    "# Tworzenie chmury s≈Ç√≥w\n",
    "wordcloud = WordCloud(width=800, height=400,\n",
    "                      background_color='white',\n",
    "                      colormap='coolwarm').generate_from_frequencies(token_shap_dict)\n",
    "\n",
    "# Wy≈õwietlenie chmury\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title(f\"SHAP Word Cloud - tekst {example_index}\")\n",
    "plt.show()\n"
   ],
   "id": "47cfd76b7eeaafb8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T22:20:46.781810Z",
     "start_time": "2025-11-09T22:19:50.722277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# 1. WCZYTANIE DANYCH\n",
    "# --------------------------------------------------------------------------\n",
    "df = pd.read_csv(\"HealthStory_cleaned.csv\")\n",
    "print(\"Wczytano plik HealthStory_cleaned.csv\")\n",
    "print(f\"Liczba rekord√≥w: {len(df)}\")\n",
    "print(f\"Kolumny: {list(df.columns)}\")\n",
    "\n",
    "criteria_cols = [f\"C{i}\" for i in range(1, 11)]\n",
    "missing_cols = [c for c in criteria_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Brakuje kolumn: {missing_cols}\")\n",
    "\n",
    "# Dane wej≈õciowe i wyj≈õciowe\n",
    "X_texts = df[\"text\"].astype(str).tolist()\n",
    "y_criteria = df[criteria_cols].values.astype(int)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 2. PODZIA≈Å NA TRAIN/TEST\n",
    "# --------------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_texts, y_criteria, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üîπ Trening: {len(X_train)} pr√≥bek\")\n",
    "print(f\"üîπ Test: {len(X_test)} pr√≥bek\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 3. TOKENIZACJA BEZ OGRANICZE≈É\n",
    "# --------------------------------------------------------------------------\n",
    "tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_len = max(len(seq) for seq in tokenizer.texts_to_sequences(X_train))\n",
    "\n",
    "X_train_seq = pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=max_len, padding='post')\n",
    "X_test_seq = pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=max_len, padding='post')\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 4. MODEL CNN MULTI-TASK\n",
    "# --------------------------------------------------------------------------\n",
    "EMBEDDING_DIM = 300  \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='sigmoid')  # 10 wyj≈õƒá = 10 kryteri√≥w\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=Adam(1e-3),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 5. TRENING\n",
    "# --------------------------------------------------------------------------\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train,\n",
    "    validation_data=(X_test_seq, y_test),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 6. PREDYKCJE\n",
    "# --------------------------------------------------------------------------\n",
    "y_pred_test = (model.predict(X_test_seq) > 0.5).astype(int)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 7. OCENA DLA KA≈ªDEGO KRYTERIUM\n",
    "# --------------------------------------------------------------------------\n",
    "criteria_names = [\n",
    "    \"C1: Koszty interwencji\", \"C2: Korzy≈õci\", \"C3: Zagro≈ºenia\",\n",
    "    \"C4: Jako≈õƒá dowod√≥w\", \"C5: Choroba\", \"C6: ≈πr√≥d≈Ça\",\n",
    "    \"C7: Alternatywy\", \"C8: Dostƒôpno≈õƒá\", \"C9: Innowacja\", \"C10: Konflikty\"\n",
    "]\n",
    "\n",
    "print(\"\\n WYNIKI DLA KA≈ªDEGO KRYTERIUM:\\n\")\n",
    "print(f\"{'Kryterium':<45} {'Accuracy':>10} {'F1':>10} {'Recall':>10}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "for i, name in enumerate(criteria_names):\n",
    "    y_true = y_test[:, i]\n",
    "    y_pred = y_pred_test[:, i]\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    results.append((acc, f1, rec))\n",
    "    print(f\"{name:<45} {acc:>10.4f} {f1:>10.4f} {rec:>10.4f}\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 8. ≈öREDNIE WYNIKI\n",
    "# --------------------------------------------------------------------------\n",
    "avg_acc = np.mean([r[0] for r in results])\n",
    "avg_f1 = np.mean([r[1] for r in results])\n",
    "avg_rec = np.mean([r[2] for r in results])\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"{'≈öREDNIA (wszystkie kryteria)':<45} {avg_acc:>10.4f} {avg_f1:>10.4f} {avg_rec:>10.4f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 9. Zapis wynik√≥w do pliku JSON\n",
    "# --------------------------------------------------------------------------\n",
    "results_summary = {\n",
    "    'dataset': 'HealthStory',\n",
    "    'n_samples': len(df),\n",
    "    'vocab_size': len(tokenizer.word_index),\n",
    "    'model': 'CNN Multi-Output',\n",
    "    'parameters': {\n",
    "        'embedding_dim': EMBEDDING_DIM,\n",
    "        'epochs': 10,\n",
    "        'batch_size': 64\n",
    "    },\n",
    "    'metrics_per_criterion': [],\n",
    "    'average_metrics': {\n",
    "        'accuracy': float(avg_acc),\n",
    "        'f1': float(avg_f1),\n",
    "        'recall': float(avg_rec)\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, name in enumerate(criteria_names):\n",
    "    results_summary['metrics_per_criterion'].append({\n",
    "        'criterion': name,\n",
    "        'accuracy': float(results[i][0]),\n",
    "        'f1': float(results[i][1]),\n",
    "        'recall': float(results[i][2])\n",
    "    })\n",
    "\n",
    "with open('results_CNN_MultiOutput.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"\\nWyniki zapisane do: results_CNN_MultiOutput.json\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# 10. ZAPIS MODELU I TOKENIZERA\n",
    "# --------------------------------------------------------------------------\n",
    "import pickle\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Zapis modelu Keras\n",
    "model.save(\"CNN_MultiOutput_model.h5\")\n",
    "print(\"Model zapisany do: CNN_MultiOutput_model.h5\")\n",
    "\n",
    "# Zapis tokenizer\n",
    "with open(\"tokenizer_MultiOutput.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "print(\"Tokenizer zapisany do: tokenizer.pkl\")\n",
    "\n"
   ],
   "id": "f440580b75d8a3c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Wczytano plik HealthStory_cleaned.csv\n",
      "Liczba rekord√≥w przed ograniczeniem: 1638\n",
      "Kolumny: ['news_id', 'title', 'text', 'rating', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'binary_label']\n",
      "üîπ Liczba rekord√≥w po ograniczeniu do 200: 200\n",
      "üîπ Trening: 160 pr√≥bek\n",
      "üîπ Test: 40 pr√≥bek\n",
      "üìö Liczba unikalnych s≈Ç√≥w w korpusie: 6883\n",
      "üìè Najd≈Çu≈ºszy tekst ma d≈Çugo≈õƒá: 1471 token√≥w\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 1471, 300)         2064900   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1467, 128)         192128    \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,274,830\n",
      "Trainable params: 2,274,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 8s 2s/step - loss: 0.6842 - accuracy: 0.0437 - val_loss: 0.6716 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6417 - accuracy: 0.0125 - val_loss: 0.6539 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.6163 - accuracy: 0.0500 - val_loss: 0.6412 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5861 - accuracy: 0.0625 - val_loss: 0.6346 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5667 - accuracy: 0.0500 - val_loss: 0.6351 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5585 - accuracy: 0.0812 - val_loss: 0.6360 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5420 - accuracy: 0.1000 - val_loss: 0.6340 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5396 - accuracy: 0.0750 - val_loss: 0.6300 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5331 - accuracy: 0.0688 - val_loss: 0.6260 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5142 - accuracy: 0.1312 - val_loss: 0.6243 - val_accuracy: 0.0000e+00\n",
      "2/2 [==============================] - 1s 100ms/step\n",
      "\n",
      "üìä WYNIKI DLA KA≈ªDEGO KRYTERIUM:\n",
      "\n",
      "Kryterium                                       Accuracy         F1     Recall\n",
      "================================================================================\n",
      "C1: Koszty interwencji                            0.7750     0.0000     0.0000\n",
      "C2: Korzy≈õci                                      0.6500     0.0000     0.0000\n",
      "C3: Zagro≈ºenia                                    0.6500     0.0000     0.0000\n",
      "C4: Jako≈õƒá dowod√≥w                                0.6500     0.0000     0.0000\n",
      "C5: Choroba                                       0.8250     0.9041     1.0000\n",
      "C6: ≈πr√≥d≈Ça                                        0.6500     0.0000     0.0000\n",
      "C7: Alternatywy                                   0.6000     0.7500     0.9231\n",
      "C8: Dostƒôpno≈õƒá                                    0.6750     0.8060     1.0000\n",
      "C9: Innowacja                                     0.6000     0.7500     1.0000\n",
      "C10: Konflikty                                    0.7750     0.8732     1.0000\n",
      "================================================================================\n",
      "≈öREDNIA (wszystkie kryteria)                      0.6850     0.4083     0.4923\n",
      "================================================================================\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
